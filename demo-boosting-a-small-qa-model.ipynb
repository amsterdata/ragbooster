{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5e145e",
   "metadata": {},
   "source": [
    "# RAGBooster\n",
    "\n",
    "We detail how to improve the performance for data imputation with retrieval augmentation and learned data pruning. We showcase how these techniques allow a tiny model to perform within 5% accuracy range of a large commercial LLM with 175 billion parameters.\n",
    "\n",
    "Our library \"RAGBooster\" for learned data pruning is available as open source.\n",
    "\n",
    "### Setup\n",
    "\n",
    "In order to run this demo, we need access to GPT3.5 and the Bing web API. Note that we implement caching and can serve the vast majority of requests from our cache for this particular demo setup.\n",
    "\n",
    " 1. **Access to GPT3.5 from OpenAI**: This demo notebook leverages GPT3.5 via the OpenAI API. It requires you to make your [OpenAI API key](https://platform.openai.com/account/api-keys) available as an environment variable via the following command:<br/><br/>`export OPENAI_API_KEY=your_secret_openai_key`<br/><br/>\n",
    " \n",
    " 1. **Access to the Bing Websearch API**: Furthermore, we will query the web via [Microsoft Bing's websearch API](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api). You need to make your Bing API key available as an environment variable via the following command:<br/><br/>`export BING_SUBSCRIPTION_KEY=your_secret_bing_key`<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d71b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ragbooster import BingRetriever, HuggingfaceQAGenerator, Generator, RetrievalAugmentedModel, RAGBooster, score\n",
    "from ragbooster.demo import load_imputation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb0c3ac",
   "metadata": {},
   "source": [
    "We leverage a tabular dataset about restaurants, where the task is to impute the `city` attribute \n",
    "based on the `name`, `address` and `phone` number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c63d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "questions = load_imputation_dataset('demo_data/restaurant.csv', \n",
    "                                    impute='city', \n",
    "                                    based_on=['name', 'address', 'phone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21188f46",
   "metadata": {},
   "source": [
    "The first question concerns a restaurant from los angeles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ad51c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question(text='name: border grill; address: 4th st.; phone: 310/451-1655', correct_answers=['los angeles'], metadata={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_questions, test_questions = train_test_split(questions, test_size=0.5)\n",
    "validation_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0c186",
   "metadata": {},
   "source": [
    "In this demo, we showcase how far we can boost the performance of small model designed for question answering. In particular, we use the [deepset/minilm-uncased-squad2](https://huggingface.co/deepset/minilm-uncased-squad2) model, and extend the `HuggingfaceQAGenerator` class to generate predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9ab266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAGenerator(HuggingfaceQAGenerator):\n",
    "    \n",
    "    def __init__(self, model_name, cache_path):\n",
    "        super().__init__(model_name, cache_path)\n",
    "    \n",
    "    def _create_prompt(self, question, params):\n",
    "        return { 'question': \"What is the name of the city in which this restaurant is located?\",\n",
    "                 'context': question.text }\n",
    "    \n",
    "    def _extract_answer(self, response):\n",
    "        return response['answer'].lower()\n",
    "    \n",
    "minilm = QAGenerator('deepset/minilm-uncased-squad2', 'demo_data/qa-cache.pkl')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204614db",
   "metadata": {},
   "source": [
    "Out of the box, this model performs pretty bad on our imputation task and only manages to predict \n",
    "less than 6% of the cities correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af906e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The accuracy of minilm is 0.05555555555555555.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = score(test_questions, minilm)\n",
    "\n",
    "f'The accuracy of minilm is {accuracy}.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527214a9",
   "metadata": {},
   "source": [
    "### GPT3.5\n",
    "\n",
    "Let's see how well GPT3.5 is doing on this task. We implement a custom `Generator` for GPT3.5, which uses a few shot prompt generated from the first five samples of the validation data (in the format proposed in a recent VLDB paper on [Can Foundation Models Wrangle Your Data?](https://www.vldb.org/pvldb/vol16/p738-narayan.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecfc292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from manifest import Manifest \n",
    "\n",
    "class FewShotGenerator(Generator):\n",
    "    \n",
    "    FEW_SHOT_PROMPT = \"name: border grill; address: 4th st.; phone: 310/451-1655; city? los angeles\\n\\n\"+\\\n",
    "        \"name: le soleil; address: 133 clement st.; phone: 415/668-4848; city? san francisco\\n\\n\"+\\\n",
    "        \"name: cypress club; address: 500 jackson st.; phone: 415/296-8555; city? san francisco\\n\\n\"+\\\n",
    "        \"name: west; address: 63rd street steakhouse 44 w. 63rd st.; phone: 212/246-6363; city? new york\\n\\n\"+\\\n",
    "        \"name: schatzi on main; address: 3110 main st.; phone: 310/399-4800; city? los angeles\\n\\n\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(llm=llm, max_tokens=10)    \n",
    "    \n",
    "    def _create_prompt(self, question, params):        \n",
    "        return f\"{self.FEW_SHOT_PROMPT}{question.text}; city?\"   \n",
    "    \n",
    "    def _extract_answer(self, response):\n",
    "        answer = response.get_response()          \n",
    "\n",
    "        answer = re.sub(r'[0-9]+', '', answer)\n",
    "        answer = answer.strip()   \n",
    "\n",
    "        for sep in ['\\n', ',', '.']:\n",
    "            if sep in answer:\n",
    "                answer = answer.split(sep)[0]\n",
    "\n",
    "        return answer.strip()  \n",
    "    \n",
    "\n",
    "\n",
    "gpt35_client = Manifest(client_name=\"openai\", engine=\"text-davinci-003\",\n",
    "                        cache_name=\"sqlite\", cache_connection=\"demo_data/gpt35-cache.sqlite\")\n",
    "\n",
    "gpt35 = FewShotGenerator(llm=gpt35_client)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9ae93",
   "metadata": {},
   "source": [
    "GPT3.5 performs astonishingly well on this task, and imputes nearly 90% of the cities correctly. (Note that GPT3.5 has most certainly seen the data at training time, as it is a common evaluation dataset in academic research)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c714c648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The accuracy of GPT3.5 is 0.8981481481481481.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = score(test_questions, gpt35)\n",
    "\n",
    "f'The accuracy of GPT3.5 is {accuracy}.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0df849",
   "metadata": {},
   "source": [
    "## Retrieval Augmentation with Bing Websearch\n",
    "\n",
    "We can improve the performance of our tiny model by providing it with some external data, for example from the web. This is called retrieval augmentation, and we use Microsoft's Bing websearch API for that by extending the `BingRetriever` class and defining how to create a query from the question text. In our case, we can just use the question text as the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800d0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBingWebsearch(BingRetriever):\n",
    "    \n",
    "    def __init__(self, cache_path):\n",
    "        super().__init__(cache_path)\n",
    "    \n",
    "    def create_query(self, question):\n",
    "        return question.text\n",
    "    \n",
    "bing_websearch = MyBingWebsearch('demo_data/bing-cache.pkl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4d3c7",
   "metadata": {},
   "source": [
    "Let's look at an example to impute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f43f3bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question(text=\"name: scala's bistro; address: 432 powell st.; phone: 415/395-8555\", correct_answers=['san francisco'], metadata={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = validation_questions[11]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e241a",
   "metadata": {},
   "source": [
    "Data from the web helps greatly here, the correct city 'san francisco' for this example is already contained in the top three answers retrieved from Bing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c89cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tableagent.com/san-francisco/scalas-bistro/ - Reservations Scala's Bistro Reservations Date Time Party Size Business Info + − Leaflet | © OpenStreetMap Address: 432 Powell Street, San Francisco CA 94102 Cross Street: Post Street Location: San Francisco | Union Square Cuisine: French | Italian | Pasta | Cost: | Moderate Category: Fine Dining Star Rating: Reservations: Unknown \n",
      "\n",
      "https://www.yellowpages.com/san-francisco-ca/mip/scalas-bistro-4887204 - ﻿ $$$ Italian Restaurants, Bars, Continental Restaurants (2) (2076) 7.1 OPEN NOW Today: 8:00 am - 11:00 pm 21 YEARS IN BUSINESS Amenities: (415) 395-8555 Map & Directions 432 Powell StSan Francisco, CA 94102 Write a Review Is this your business? Customize this page. Claim This Business Hours Regular Hours Scala's Bistro 432 Powell St, San Francisco \n",
      "\n",
      "https://www.chamberofcommerce.com/united-states/california/san-francisco/italian-restaurant/2006879304-scala-s-bistro - Scala's Bistro at 432 Powell St, San Francisco, CA 94102. Get Scala's Bistro can be contacted at (415) 395-8555. Get Scala's Bistro reviews, rating, hours, phone number, directions and more. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieved = bing_websearch.retrieve(example)\n",
    "for snippet, url in retrieved[:3]:\n",
    "    print(url, '-', snippet, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12710eb8",
   "metadata": {},
   "source": [
    "In order to leverage the web data, we implement a new `Generator`, which uses the retrieved text from the web for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87425e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAGeneratorWithContext(HuggingfaceQAGenerator):\n",
    "    \n",
    "    def __init__(self, model_name, cache_path):\n",
    "        super().__init__(model_name, cache_path)\n",
    "    \n",
    "    def _create_prompt(self, question, params):\n",
    "        retrieved_context = params['retrieved_context']\n",
    "        return { 'question': \"What is the name of the city in which this restaurant is located?\",\n",
    "                 'context': f'{retrieved_context};{question.text}' }\n",
    "    \n",
    "    def _extract_answer(self, response):\n",
    "        return response['answer'].lower()\n",
    "    \n",
    "minilm_ctx = QAGeneratorWithContext('deepset/minilm-uncased-squad2', 'demo_data/qa_ctx-cache.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7222e46",
   "metadata": {},
   "source": [
    "If we enhance our minilm with retrieval augmentation using `k=3` answers, it correctly imputes our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a6f833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'san francisco'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag3 = RetrievalAugmentedModel(bing_websearch, minilm_ctx, k=3)\n",
    "rag3.generate(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6234b2",
   "metadata": {},
   "source": [
    "If we use retrieval augmentation and set `k=10`, our tiny model already achieves a performance of over 80%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f6dd996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The accuracy with retrieval augmentation and k=10 on the test set is 0.8009259259259259'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag10 = RetrievalAugmentedModel(bing_websearch, minilm_ctx, k=10)\n",
    "\n",
    "accuracy_rag_10 = score(test_questions, rag10)\n",
    "\n",
    "f'The accuracy with retrieval augmentation and k=10 on the test set is {accuracy_rag_10}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e9be0",
   "metadata": {},
   "source": [
    "## Improving the performance further with RAGBooster\n",
    "\n",
    "We can further improve the performance of our retrieval-augmented model by learning the data importance of the retrieval sources (web domains in our case) and pruning the retrieval corpus accordingly. Checkout our recent paper on **Improving Retrieval-Augmented Large Language Models with Data-Centric Refinement** (TODO need arxiv version) for details on the algorithm behind this.\n",
    "\n",
    "We can \"boost\" the performance of our model via the `RAGBooster` class and an additional set of validation questions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d896ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "refined_rag_model = RAGBooster(rag10, validation_questions[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e083da7",
   "metadata": {},
   "source": [
    "This \"boosting\" improves accuracy by more than 4% and brings us within about 5% of the performance achieved by the commercial LLM GPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91dab38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'RAGBooster improved the accuracy with retrieval augmentation by 0.044 to 0.8449074074074074!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_refined = score(test_questions, refined_rag_model)\n",
    "improvement = accuracy_refined - accuracy_rag_10\n",
    "\n",
    "f'RAGBooster improved the accuracy with retrieval augmentation by {improvement:.3f}'+\\\n",
    "f' to {accuracy_refined}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fcc17e",
   "metadata": {},
   "source": [
    "We can finally inspect the most important data sources (domains), which RAGBooster identifies in our retrieval corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c752b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hpi.de', 1.0),\n",
       " ('researchgate.net', 1.0),\n",
       " ('wu-wien.ac.at', 1.0),\n",
       " ('folkd.com', 0.9381223189772298),\n",
       " ('lasvegascasinos.com', 0.9231084964258172)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains_and_weights = refined_rag_model.weights\n",
    "domains_and_weights_sorted = sorted(domains_and_weights.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "domains_and_weights_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22c6ad",
   "metadata": {},
   "source": [
    "Interestingly, the website of the Hasso Plattner Institute is among the top sources, which incidentally contains a dirty version of the actual restaurants data:\n",
    "\n",
    "https://hpi.de/naumann/projects/repeatability/datasets/restaurants-dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e85db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
